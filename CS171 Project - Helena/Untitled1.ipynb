{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9d8fcf-bc58-421a-9879-6a6ec2b91e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "try:\n",
    "    # much faster than PIL on many systems (libjpeg-turbo)\n",
    "    from torchvision.io import read_image\n",
    "    _HAS_TV_READ = True\n",
    "except Exception:\n",
    "    _HAS_TV_READ = False\n",
    "\n",
    "\n",
    "class RCNN_Warp_Data(Dataset):\n",
    "    \"\"\"\n",
    "    Drop-in replacement focused on speed:\n",
    "      - pre-parses label files once in __init__\n",
    "      - per-sample: decode image + scale cached boxes\n",
    "      - works with (image, target) transforms\n",
    "    \"\"\"\n",
    "    def __init__(self, root_path, split, transforms=None, class_path=\"classes.txt\",\n",
    "                 use_tv_read=True):\n",
    "        self.root_path = root_path\n",
    "        self.split_path = split\n",
    "        self.class_path = class_path\n",
    "        self.transforms = transforms\n",
    "        self.use_tv_read = (use_tv_read and _HAS_TV_READ)\n",
    "\n",
    "        # ---- resolve paths once ----\n",
    "        self.class_names = self.__get_classes()\n",
    "        self.image_paths, self.label_paths = self.__get_paths()\n",
    "\n",
    "        # ---- pre-parse all label files (YOLO format) once ----\n",
    "        # cache normalized (cx,cy,w,h) + labels per index to avoid file I/O in __getitem__\n",
    "        self._norm_targets = []   # list of (labels_np[int64], boxes_np[float32 Nx4 in cxcywh])\n",
    "        for lp in self.label_paths:\n",
    "            try:\n",
    "                if not os.path.exists(lp) or os.path.getsize(lp) == 0:\n",
    "                    self._norm_targets.append((None, None))\n",
    "                    continue\n",
    "                # robust, vectorized load; enforce 2D\n",
    "                arr = np.loadtxt(lp, ndmin=2, dtype=np.float32)\n",
    "                # if file had a single value (corrupt), skip\n",
    "                if arr.ndim != 2 or arr.shape[1] < 5:\n",
    "                    self._norm_targets.append((None, None))\n",
    "                    continue\n",
    "                labels = arr[:, 0].astype(np.int64)\n",
    "                cxcywh = arr[:, 1:5].astype(np.float32)\n",
    "                self._norm_targets.append((labels, cxcywh))\n",
    "            except Exception:\n",
    "                # keep dataset robust; treat as empty\n",
    "                self._norm_targets.append((None, None))\n",
    "\n",
    "    # ------------- original helpers -------------\n",
    "    def __get_classes(self):\n",
    "        with open(os.path.join(self.root_path, self.class_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            names = [ln.strip() for ln in f if ln.strip()]\n",
    "        return names\n",
    "\n",
    "    def __get_paths(self):\n",
    "        # expects standard YOLO-like layout: <root>/<split>/images & <root>/<split>/labels\n",
    "        img_dir = os.path.join(self.root_path, self.split_path, \"images\")\n",
    "        lbl_dir = os.path.join(self.root_path, self.split_path, \"labels\")\n",
    "\n",
    "        imgs, lbls = [], []\n",
    "        for fn in sorted(os.listdir(img_dir)):\n",
    "            if not fn.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                continue\n",
    "            img_p = os.path.join(img_dir, fn)\n",
    "            lbl_p = os.path.join(lbl_dir, os.path.splitext(fn)[0] + \".txt\")\n",
    "            imgs.append(img_p)\n",
    "            lbls.append(lbl_p)\n",
    "        return imgs, lbls\n",
    "\n",
    "    # ------------- dataset API -------------\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # ---- fast decode image ----\n",
    "        img_path = self.image_paths[index]\n",
    "        if self.use_tv_read:\n",
    "            # CHW uint8 -> float 0..1\n",
    "            img = read_image(img_path).float() / 255.0\n",
    "            # If your transforms expect PIL, uncomment the next line:\n",
    "            # img = F.to_pil_image(img)\n",
    "        else:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # get width/height (no extra open if using tensor)\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # tensor: [C,H,W]\n",
    "            H, W = int(img.shape[1]), int(img.shape[2])\n",
    "        else:\n",
    "            W, H = img.size\n",
    "\n",
    "        # ---- build target using cached normalized boxes ----\n",
    "        labels_np, cxcywh_np = self._norm_targets[index]\n",
    "        if labels_np is None or cxcywh_np is None or len(labels_np) == 0:\n",
    "            boxes_t  = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels_t = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            # vectorized conversion from normalized YOLO (cx,cy,w,h) -> xyxy in pixels\n",
    "            cx = cxcywh_np[:, 0] * W\n",
    "            cy = cxcywh_np[:, 1] * H\n",
    "            ww = cxcywh_np[:, 2] * W\n",
    "            hh = cxcywh_np[:, 3] * H\n",
    "            x1 = cx - ww / 2.0\n",
    "            y1 = cy - hh / 2.0\n",
    "            x2 = cx + ww / 2.0\n",
    "            y2 = cy + hh / 2.0\n",
    "            # clamp and ensure positive area\n",
    "            eps = 1e-3\n",
    "            x1 = np.clip(x1, 0, W - 1)\n",
    "            y1 = np.clip(y1, 0, H - 1)\n",
    "            x2 = np.clip(x2, x1 + eps, W - 1)\n",
    "            y2 = np.clip(y2, y1 + eps, H - 1)\n",
    "\n",
    "            boxes_t  = torch.from_numpy(np.stack([x1, y1, x2, y2], axis=1)).to(torch.float32)\n",
    "            labels_t = torch.from_numpy(labels_np).to(torch.int64)\n",
    "\n",
    "        area_t = (boxes_t[:, 2] - boxes_t[:, 0]) * (boxes_t[:, 3] - boxes_t[:, 1])\n",
    "        target = {\n",
    "            \"boxes\":   boxes_t,\n",
    "            \"labels\":  labels_t,\n",
    "            \"image_id\": torch.tensor([index], dtype=torch.int64),\n",
    "            \"area\":    area_t,\n",
    "            \"iscrowd\": torch.zeros((boxes_t.shape[0],), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        # ---- transforms (support (img, target) or img-only) ----\n",
    "        if self.transforms is not None:\n",
    "            try:\n",
    "                img, target = self.transforms(img, target)\n",
    "            except TypeError:\n",
    "                img = self.transforms(img)\n",
    "\n",
    "        # ensure tensor image for FasterRCNN\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = F.to_tensor(img)\n",
    "        elif isinstance(img, torch.Tensor) and img.dtype != torch.float32:\n",
    "            img = img.float()\n",
    "\n",
    "        target[\"labels\"] = target[\"labels\"].to(torch.int64).reshape(-1)\n",
    "        target[\"boxes\"]  = target[\"boxes\"].to(torch.float32).reshape(-1, 4)\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b8c94c8-c5fc-42b9-982e-9dab2951aa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "out: 64  hidden: 32 depth: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 197\u001b[39m\n\u001b[32m    192\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m-----------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m testParams()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 190\u001b[39m, in \u001b[36mtestParams\u001b[39m\u001b[34m(out_channels, hidden_sizes, depth, NUM_EPOCHS, num_classes)\u001b[39m\n\u001b[32m    188\u001b[39m optimizer = optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  hidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m depth: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt[\u001b[32m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m train_losses, test_map, train_map = training_loop(model, optimizer, NUM_EPOCHS, train_loader, test_loader, printing=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    191\u001b[39m res += (train_losses, test_map, train_map)\n\u001b[32m    192\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m-----------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 129\u001b[39m, in \u001b[36mtraining_loop\u001b[39m\u001b[34m(model, optimizer, NUM_EPOCHS, train_loader, test_loader, printing)\u001b[39m\n\u001b[32m    127\u001b[39m model.eval()\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     preds = model(train_inputs)  \u001b[38;5;66;03m# Now it returns predictions\u001b[39;00m\n\u001b[32m    130\u001b[39m     train_metric.update(preds, train_labels)\n\u001b[32m    131\u001b[39m model.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:101\u001b[39m, in \u001b[36mGeneralizedRCNN.forward\u001b[39m\u001b[34m(self, images, targets)\u001b[39m\n\u001b[32m     94\u001b[39m             degen_bb: List[\u001b[38;5;28mfloat\u001b[39m] = boxes[bb_idx].tolist()\n\u001b[32m     95\u001b[39m             torch._assert(\n\u001b[32m     96\u001b[39m                 \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     97\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mAll bounding boxes should have positive height and width.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     98\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Found invalid box \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdegen_bb\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for target at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     99\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m features = \u001b[38;5;28mself\u001b[39m.backbone(images.tensors)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch.Tensor):\n\u001b[32m    103\u001b[39m     features = OrderedDict([(\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m, features)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mCustom_Backbone.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     98\u001b[39m     \u001b[38;5;66;03m# return a single Tensor feature map\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.body(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = module(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    550\u001b[39m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m.stride, \u001b[38;5;28mself\u001b[39m.padding, \u001b[38;5;28mself\u001b[39m.dilation, \u001b[38;5;28mself\u001b[39m.groups\n\u001b[32m    551\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torchvision.ops import nms\n",
    "import itertools\n",
    "from torch.utils.data import random_split\n",
    "import json\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "import traceback\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" ##my computer has an installation issue, will fix and remove this from final version\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "root = Path.cwd() / \"Warp-D\"\n",
    "\n",
    "##temporary replacement for above cell\n",
    "train_ds = RCNN_Warp_Data(root, split=\"train\")\n",
    "test_ds  = RCNN_Warp_Data(root, split=\"test\")\n",
    "\n",
    "sub_size = 0.25\n",
    "train_size = int(sub_size * len(train_ds))\n",
    "test_size = int(sub_size * len(test_ds))\n",
    "\n",
    "train_ds, _ = random_split(train_ds, [train_size, len(train_ds) - train_size]) \n",
    "test_ds, _ = random_split(test_ds, [test_size, len(test_ds) - test_size]) \n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=2,           \n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "##basic starter to prove it works, will improve later\n",
    "class Custom_Backbone(nn.Module):\n",
    "    def __init__ (self, in_channels = 3, out_channels = 256, hidden_size = 32, depth = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = in_channels\n",
    "        out_dim = hidden_size\n",
    "\n",
    "        for i in range(depth):\n",
    "            layers += [\n",
    "                nn.Conv2d(in_dim, out_dim, 3, stride=2, padding=1),  \n",
    "                nn.BatchNorm2d(out_dim), nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_dim = out_dim\n",
    "            out_dim = min(out_dim * 2, 1024)\n",
    "\n",
    "        layers += [\n",
    "            nn.Conv2d(in_dim, out_channels, 3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),    \n",
    "        ]\n",
    "            \n",
    "        self.body = nn.Sequential(*layers)\n",
    "        # required by FasterRCNN\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return a single Tensor feature map\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "def training_loop(model, optimizer, NUM_EPOCHS, train_loader, test_loader, printing=True): \n",
    "    test_metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "    train_metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "    test_maps = []\n",
    "    train_losses = []\n",
    "    train_maps = []\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        \n",
    "        # Run the training loop\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for train_inputs, train_labels in train_loader:\n",
    "            train_inputs = list(image.to(device) for image in train_inputs)\n",
    "            train_labels = [{k: v.to(device) for k, v in t.items()} for t in train_labels]\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_loss_dict = model(train_inputs, train_labels)\n",
    "            loss = sum(train_loss_dict.values())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds = model(train_inputs)  # Now it returns predictions\n",
    "                train_metric.update(preds, train_labels)\n",
    "            model.train()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_map_results = train_metric.compute()\n",
    "        train_maps.append(train_map_results['map'])\n",
    "\n",
    "        \n",
    "\n",
    "        # Run the testing loop\n",
    "        # this is essentially the same as the training loop but\n",
    "        # without the optimizer and backward propagation\n",
    "        #model.eval()\n",
    "        total_test_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for test_inputs, test_labels in test_loader:\n",
    "                test_inputs = list(image.to(device) for image in test_inputs)\n",
    "                test_labels = [{k: v.to(device) for k, v in t.items()} for t in test_labels]\n",
    "                \n",
    "                predictions = model(test_inputs)\n",
    "                test_metric.update(predictions, test_labels)\n",
    "            #     loss = sum(test_loss_dict.values())\n",
    "            \n",
    "            #     total_test_loss += loss.item()\n",
    "            test_map_results = test_metric.compute()\n",
    "            # avg_test_loss = total_test_loss / len(test_loader)\n",
    "            test_maps.append(test_map_results['map'])\n",
    "            test_metric.reset()\n",
    "\n",
    "        if printing:\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\"+\\\n",
    "                  f\" - Train Loss: {avg_train_loss:.4f} \", end = \" \")\n",
    "            print(f\"- Train Map {train_map_results['map'].item():.4f}\", end = \" \")\n",
    "            print(f\"- Test  Map {test_map_results['map'].item():.4f}\")\n",
    "\n",
    "    return train_losses,test_maps, train_maps\n",
    "\n",
    "\n",
    "def testParams(out_channels = [64, 128, 256, 512], hidden_sizes = [32, 64, 128, 256, 512], depth = [1, 2, 3, 4, 5], NUM_EPOCHS = 5, num_classes = 29):\n",
    "    combinations = list(itertools.product(out_channels, hidden_sizes, depth))\n",
    "    models = []\n",
    "    res = []\n",
    "\n",
    "    anchor_sizes = ((32, 64, 128, 256, 512),) \n",
    "    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "    rpn_anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
    "\n",
    "    for t in combinations:\n",
    "        custom_backbone = Custom_Backbone(3, t[0], t[1], t[2])\n",
    "        model = FasterRCNN(\n",
    "            backbone=custom_backbone,\n",
    "            num_classes=num_classes,\n",
    "            rpn_anchor_generator=rpn_anchor_generator,\n",
    "            )\n",
    "        model = model.to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        print(f\"out: {t[0]}  hidden: {t[1]} depth: {t[2]}\")\n",
    "        train_losses, test_map, train_map = training_loop(model, optimizer, NUM_EPOCHS, train_loader, test_loader, printing=True)\n",
    "        res += (train_losses, test_map, train_map)\n",
    "        print(\"\\n-----------\\n\")\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "testParams()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs171",
   "language": "python",
   "name": "cs171"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
