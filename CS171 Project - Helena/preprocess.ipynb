{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704e8b70-b067-4554-abd6-66fe1764503e",
   "metadata": {},
   "source": [
    "# CS171 Project\n",
    "## Data Preprocessing\n",
    "### Nothing in this notebook is finalized or in its final format\n",
    "**Author** - Helena Thiessen\n",
    "\n",
    "**Date** - Nov 20/25\n",
    "\n",
    "### Motivation\n",
    "Preparing data for machine learning with the goal of using an R-CNN to identify recyclable objects among waste to aid in recycling sorting facilities. Preparing the Warp dataset downloaded from Kaggle for machine learning and constructing a validation set from personally sourced images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228526b-9edd-40e7-acac-307eda04b3db",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9c532e4-e675-49ea-8b41-4db8ce27c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "import traceback\n",
    "from pycocotools.coco import COCO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1805bfeb-b8f7-4469-bb3a-c32b29734b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffdfa37-608c-4ff3-b464-b338eb71f5bd",
   "metadata": {},
   "source": [
    "## Pre-Processing For Warp Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f01b4113-9816-4bee-a7f5-e63e3258999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN_Warp_Data(Dataset):\n",
    "    def __init__(self, root_path, split, transforms=None):\n",
    "\n",
    "        ##get the paths for reading the data\n",
    "        ##replace if you change any structure or names\n",
    "        self.root_path = root_path\n",
    "        self.split_path = split\n",
    "        self.class_path = \"classes.txt\"\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.class_names = self.__get_classes()  \n",
    "        self.image_paths, self.label_paths = self.__get_paths()            \n",
    "\n",
    "    def __get_paths(self):\n",
    "        img_dir  = self.root_path / self.split_path / \"images\"\n",
    "        label_dir = self.root_path / self.split_path / \"labels\"\n",
    "        \n",
    "        img_paths = [p for p in img_dir.iterdir()]\n",
    "        label_paths = [label_dir / (p.stem + \".txt\") for p in img_paths]\n",
    "\n",
    "        return img_paths, label_paths\n",
    "\n",
    "    \n",
    "    def __get_classes(self):\n",
    "        ##Get the classes and properly associate them to the labels\n",
    "        classes = []\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(self.root_path, self.class_path), 'r') as class_file:\n",
    "                for line in class_file:\n",
    "                    classes.append(line.strip())\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: The class file was not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[CLASSES ERROR] {self.class_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def __read_labels(self, file_path, W, H):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(self.root_path, self.split_path, \"labels\", file_path), 'r') as label_file:\n",
    "                for line in label_file:\n",
    "                    s = line.strip()\n",
    "\n",
    "                    if not s or s.startswith(\"#\"):            # <- skip blank/comment\n",
    "                        continue\n",
    "                    parts = s.split()\n",
    "                    if len(parts) < 5:                         # <- malformed line\n",
    "                        print(f\"[BAD LINE] {full}:{ln} -> {s!r}\")\n",
    "                        continue\n",
    "                    \n",
    "                    parts = line.split()\n",
    "                    item_class = int(parts[0])\n",
    "                    cx, cy, w, h = map(float, parts[1:5])\n",
    "                    x_c, y_c = cx * W, cy * H\n",
    "                    bw, bh   = w * W,  h * H\n",
    "                    x1 = max(0.0, x_c - bw/2)\n",
    "                    y1 = max(0.0, y_c - bh/2)\n",
    "                    x2 = min(float(W), x_c + bw/2)\n",
    "                    y2 = min(float(H), y_c + bh/2)\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(item_class + 1)\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            print(f\"[LABEL MISSING] {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[READ LABEL ERROR] {file_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        return boxes, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        file_path = self.label_paths[index]\n",
    "        img = Image.open(image_path)\n",
    "        W, H = img.size\n",
    "        boxes, labels = self.__read_labels(file_path, W, H)\n",
    "\n",
    "        boxes  = torch.as_tensor(boxes,  dtype=torch.float32) \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)  \n",
    "        area   = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) \n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([index]),\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        img = F.to_tensor(img)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "731e4e3c-5beb-488c-a4ba-dab82f432d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __create_super_class_dict(self):\n",
    "        ##Also create a super class dict so we can analyze at different levels/ combine categories\n",
    "\n",
    "        self.super_class_dict = {\"Full Plastic Bottle\" : [], \"Crushed Plastic Bottle\" : [],\n",
    "                    \"Glass Bottle\" : [], \"Detergent Container\" : [], \"Cardboard Carton\" : [], \"Metal Can\" : [], \"Other Plastic Jug\" : []}\n",
    "\n",
    "        for i in self.class_dict:\n",
    "            if \"full\" in self.class_dict[i]:\n",
    "                self.super_class_dict[\"Full Plastic Bottle\"].append(i)\n",
    "            elif \"bottle\" in self.class_dict[i]:\n",
    "                self.super_class_dict[\"Crushed Plastic Bottle\"].append(i)\n",
    "            elif \"glass\" in self.class_dict[i]:\n",
    "                self.super_class_dict[\"Glass Bottle\"].append(i)\n",
    "            elif \"detergent\" in self.class_dict[i]:\n",
    "                self.super_class_dict[\"Detergent Container\"].append(i)\n",
    "            elif \"cardboard\" in self.class_dict[i]:\n",
    "                self.super_class_dict[\"Cardboard Carton\"].append(i)\n",
    "            elif \"canister\" in self.class_dict[i]:\n",
    "                self.super_class_dict[\"Other Plastic Jug\"].append(i)\n",
    "            elif \"can\" in self.class_dict[i]:\n",
    "                self.super_class_dict[\"Metal Can\"].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6424d626-0017-4a13-a432-4b660bae88e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __create_class_dict(self):\n",
    "        ##Get the classes and properly associate them to the labels\n",
    "        self.class_dict = {}\n",
    "        i = 1\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(self.root_path, self.class_path), 'r') as class_file:\n",
    "                for line in class_file:\n",
    "                    class_dict[i] = line.strip()\n",
    "                    i += 1\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: The file was not found.\")\n",
    "        except Exception:\n",
    "            print(\"An error occurred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a6333-8ca8-4018-81c4-7effff021411",
   "metadata": {},
   "source": [
    "## We might not like warp, lets try taco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7780c0da-a87b-4946-9a46-e17238fcb9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TACODataset(Dataset):\n",
    "    def __init__(self, img_dir, annotation_file, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        self.images = self.annotations['images']\n",
    "        self.categories = self.annotations['categories']\n",
    "        self.annotations_by_image_id = self._group_annotations_by_image()\n",
    "\n",
    "    def _group_annotations_by_image(self):\n",
    "        annotations_map = {}\n",
    "        for ann in self.annotations['annotations']:\n",
    "            image_id = ann['image_id']\n",
    "            if image_id not in annotations_map:\n",
    "                annotations_map[image_id] = []\n",
    "            annotations_map[image_id].append(ann)\n",
    "        return annotations_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info['id']\n",
    "        img_filename = img_info['file_name']\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Get annotations for this image\n",
    "        anns = self.annotations_by_image_id.get(img_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            # COCO bounding box format is [x_min, y_min, width, height]\n",
    "            x_min, y_min, width, height = ann['bbox']\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([img_id])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image = F.to_tensor(image)  \n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778642fa-a20e-432d-86a8-3aa2622a6fdc",
   "metadata": {},
   "source": [
    "## Pre-processing for Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e678b1-2e42-47e7-baca-62221504b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Additional paths that are needed\n",
    "val_folder_path = \"val\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323ecc8-f99c-42e5-b67f-a29eed2169c9",
   "metadata": {},
   "source": [
    "### Description of pre-processing performed by hand\n",
    "The following pre-processing cannot be shown directly in this notebook because it was not done through code, but instead was done by hand. To illustrate these steps I have included images of the process as well as descriptions of what had to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4537e1f1-55e7-4c36-9fe9-a8cbd533266a",
   "metadata": {},
   "source": [
    "**Obtaining Images**\n",
    "\n",
    "The images in the WARP dataset followed a specific format with recylcables interspersed amongst trash in a sorting facility. To test the model made from that data I needed images that were similar enough for the model to handle and had the types of recycling that were present in the WARP classes. To assess the generalization ability of the model, I also wanted to obtain images that were formatted differently to see if it could correctly locate the recycables in different situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fba610-66ee-4765-9902-10a5ce0ed81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
