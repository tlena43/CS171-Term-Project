{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704e8b70-b067-4554-abd6-66fe1764503e",
   "metadata": {},
   "source": [
    "# CS171 Project\n",
    "## Data Preprocessing\n",
    "### Nothing in this notebook is finalized or in its final format\n",
    "**Author** - Helena Thiessen\n",
    "\n",
    "**Date** - Nov 20/25\n",
    "\n",
    "### Motivation\n",
    "Preparing data for machine learning with the goal of using an R-CNN to identify recyclable objects among waste to aid in recycling sorting facilities. Preparing the Warp dataset downloaded from Kaggle for machine learning and constructing a validation set from personally sourced images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228526b-9edd-40e7-acac-307eda04b3db",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9c532e4-e675-49ea-8b41-4db8ce27c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "import traceback\n",
    "from pycocotools.coco import COCO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1805bfeb-b8f7-4469-bb3a-c32b29734b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffdfa37-608c-4ff3-b464-b338eb71f5bd",
   "metadata": {},
   "source": [
    "## Pre-Processing For Warp Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f01b4113-9816-4bee-a7f5-e63e3258999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN_Warp_Data(Dataset):\n",
    "    def __init__(self, root_path, split, transforms=None):\n",
    "\n",
    "        ##get the paths for reading the data\n",
    "        ##replace if you change any structure or names\n",
    "        self.root_path = root_path\n",
    "        self.split_path = split\n",
    "        self.class_path = \"classes.txt\"\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.class_names = self.__get_classes()  \n",
    "        self.image_paths, self.label_paths = self.__get_paths()            \n",
    "\n",
    "    def __get_paths(self):\n",
    "        img_dir  = self.root_path / self.split_path / \"images\"\n",
    "        label_dir = self.root_path / self.split_path / \"labels\"\n",
    "        \n",
    "        img_paths = [p for p in img_dir.iterdir()]\n",
    "        label_paths = [label_dir / (p.stem + \".txt\") for p in img_paths]\n",
    "\n",
    "        return img_paths, label_paths\n",
    "\n",
    "    \n",
    "    def __get_classes(self):\n",
    "        ##Get the classes and properly associate them to the labels\n",
    "        classes = []\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(self.root_path, self.class_path), 'r') as class_file:\n",
    "                for line in class_file:\n",
    "                    classes.append(line.strip())\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: The class file was not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[CLASSES ERROR] {self.class_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def __read_labels(self, file_path, W, H):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(self.root_path, self.split_path, \"labels\", file_path), 'r') as label_file:\n",
    "                for line in label_file:\n",
    "                    s = line.strip()\n",
    "\n",
    "                    if not s or s.startswith(\"#\"):            # <- skip blank/comment\n",
    "                        continue\n",
    "                    parts = s.split()\n",
    "                    if len(parts) < 5:                         # <- malformed line\n",
    "                        print(f\"[BAD LINE] {full}:{ln} -> {s!r}\")\n",
    "                        continue\n",
    "                    \n",
    "                    parts = line.split()\n",
    "                    item_class = int(parts[0])\n",
    "                    cx, cy, w, h = map(float, parts[1:5])\n",
    "                    x_c, y_c = cx * W, cy * H\n",
    "                    bw, bh   = w * W,  h * H\n",
    "                    x1 = max(0.0, x_c - bw/2)\n",
    "                    y1 = max(0.0, y_c - bh/2)\n",
    "                    x2 = min(float(W), x_c + bw/2)\n",
    "                    y2 = min(float(H), y_c + bh/2)\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(item_class + 1)\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            print(f\"[LABEL MISSING] {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[READ LABEL ERROR] {file_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        return boxes, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        file_path = self.label_paths[index]\n",
    "        img = Image.open(image_path)\n",
    "        W, H = img.size\n",
    "        boxes, labels = self.__read_labels(file_path, W, H)\n",
    "\n",
    "        boxes  = torch.as_tensor(boxes,  dtype=torch.float32) \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)  \n",
    "        area   = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) \n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([index]),\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        img = F.to_tensor(img)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778642fa-a20e-432d-86a8-3aa2622a6fdc",
   "metadata": {},
   "source": [
    "## Pre-processing for Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e678b1-2e42-47e7-baca-62221504b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Additional paths that are needed\n",
    "val_folder_path = \"val\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323ecc8-f99c-42e5-b67f-a29eed2169c9",
   "metadata": {},
   "source": [
    "### Description of pre-processing performed by hand\n",
    "The following pre-processing cannot be shown directly in this notebook because it was not done through code, but instead was done by hand. To illustrate these steps I have included images of the process as well as descriptions of what had to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4537e1f1-55e7-4c36-9fe9-a8cbd533266a",
   "metadata": {},
   "source": [
    "**Obtaining Images**\n",
    "\n",
    "The images in the WARP dataset followed a specific format with recylcables interspersed amongst trash in a sorting facility. To test the model made from that data I needed images that were similar enough for the model to handle and had the types of recycling that were present in the WARP classes. To assess the generalization ability of the model, I also wanted to obtain images that were formatted differently to see if it could correctly locate the recycables in different situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fba610-66ee-4765-9902-10a5ce0ed81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
